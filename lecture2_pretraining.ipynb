{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "mount_file_id": "1-t9kMg-3ooVgTDQap8kqs3-bgzTSvxue",
      "authorship_tag": "ABX9TyNYEiX1q9JgG1GyZxZxjD94",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4ac24f9cd38641a8b53c6f328ccd38b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_549ec0e54cf44cea963ec1e73951f37a",
              "IPY_MODEL_3224f1d2fb614bb292b627aa428941c0",
              "IPY_MODEL_68afb72ec4a543a7a25f77fd755d5963"
            ],
            "layout": "IPY_MODEL_6156ba1b7c244448860185d36e181c48"
          }
        },
        "549ec0e54cf44cea963ec1e73951f37a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_befeeab523ae485face2ccf0b01056eb",
            "placeholder": "​",
            "style": "IPY_MODEL_8822f68f3658476ca0fc8f7e8b187d1c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3224f1d2fb614bb292b627aa428941c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42fb66364152458f93d9ba11ae3283de",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fed4b960afbe475caefac7fb5af72f1a",
            "value": 4
          }
        },
        "68afb72ec4a543a7a25f77fd755d5963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_753688b5f5b0499987b6db792155140e",
            "placeholder": "​",
            "style": "IPY_MODEL_587450c71c1c46e489809f5c95689fac",
            "value": " 4/4 [00:50&lt;00:00, 10.99s/it]"
          }
        },
        "6156ba1b7c244448860185d36e181c48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "befeeab523ae485face2ccf0b01056eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8822f68f3658476ca0fc8f7e8b187d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42fb66364152458f93d9ba11ae3283de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fed4b960afbe475caefac7fb5af72f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "753688b5f5b0499987b6db792155140e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "587450c71c1c46e489809f5c95689fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2f2a3b9fb96438a9226c002dc6b11bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b96c58b13c884834ac4a0b3d0fbe3e1e",
              "IPY_MODEL_e168e52cc5bc448a91ce69545550ea2c",
              "IPY_MODEL_7d3c635c15fb4997801b78eadd40cf4a"
            ],
            "layout": "IPY_MODEL_cd7beeb50fc34f8fbfca387ac02c24a0"
          }
        },
        "b96c58b13c884834ac4a0b3d0fbe3e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baf7244929fd44399ef409aa53d00b20",
            "placeholder": "​",
            "style": "IPY_MODEL_0afbfacddb2243739e6cd0d07cac8d67",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e168e52cc5bc448a91ce69545550ea2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_600cf339012c4e7689842d80d8b1a756",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c5ff27a423146c7b65a963a23d5abdb",
            "value": 3
          }
        },
        "7d3c635c15fb4997801b78eadd40cf4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff4ef532d1db4fa5baa5117e69c2fa94",
            "placeholder": "​",
            "style": "IPY_MODEL_191076647bb34ef0bab49e9d034d3d02",
            "value": " 3/3 [01:04&lt;00:00, 15.78s/it]"
          }
        },
        "cd7beeb50fc34f8fbfca387ac02c24a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baf7244929fd44399ef409aa53d00b20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0afbfacddb2243739e6cd0d07cac8d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "600cf339012c4e7689842d80d8b1a756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5ff27a423146c7b65a963a23d5abdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff4ef532d1db4fa5baa5117e69c2fa94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "191076647bb34ef0bab49e9d034d3d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dddbab66a75c4aa98996f07336ff868f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a188f9972d34dee968806a7dccf4a42",
              "IPY_MODEL_91194b474e6f4a3b828652fbd033cdb1",
              "IPY_MODEL_b7eef8fb5c344065b2af9ff52db02025"
            ],
            "layout": "IPY_MODEL_13914e55d8fa4cbb81d45b583b6e854e"
          }
        },
        "5a188f9972d34dee968806a7dccf4a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f731cd7e6e6244608ec7763285a6e611",
            "placeholder": "​",
            "style": "IPY_MODEL_094d505237d746f5ba485b473be57294",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "91194b474e6f4a3b828652fbd033cdb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_281ab5e1f6f94ef88ad7ee627d1f98f0",
            "max": 284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9069595e03954cb8ab06505a6958e759",
            "value": 284
          }
        },
        "b7eef8fb5c344065b2af9ff52db02025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb61f3903fc44427ba93816dd8bce377",
            "placeholder": "​",
            "style": "IPY_MODEL_0e0cf6900b0943569f4046a23eaf1d2a",
            "value": " 284/284 [00:00&lt;00:00, 23.0kB/s]"
          }
        },
        "13914e55d8fa4cbb81d45b583b6e854e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f731cd7e6e6244608ec7763285a6e611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "094d505237d746f5ba485b473be57294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "281ab5e1f6f94ef88ad7ee627d1f98f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9069595e03954cb8ab06505a6958e759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb61f3903fc44427ba93816dd8bce377": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e0cf6900b0943569f4046a23eaf1d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e7dd449d32f4693b8a422b05928d9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6227081b736943c4ad50c14a5cc22c7c",
              "IPY_MODEL_31afbe46f2044deba31e63f541c38ae4",
              "IPY_MODEL_dc731d2739e04dfb867bc2cf1fb6376a"
            ],
            "layout": "IPY_MODEL_2f4aa0bb892449b89d9c3e53e8d5bee4"
          }
        },
        "6227081b736943c4ad50c14a5cc22c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a429cc56da04fae8ea3137ad98f9c32",
            "placeholder": "​",
            "style": "IPY_MODEL_ae191a1ad48145a181fb0227e99549b7",
            "value": "spiece.model: 100%"
          }
        },
        "31afbe46f2044deba31e63f541c38ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cbd827870e5475cbfed653b255d583c",
            "max": 1341798,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99798df59275423c8d901d523e0bfa9a",
            "value": 1341798
          }
        },
        "dc731d2739e04dfb867bc2cf1fb6376a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cabad90116644ef8338456aa3382887",
            "placeholder": "​",
            "style": "IPY_MODEL_2877f8ee8c9f4ba39ade74d9dbe8095e",
            "value": " 1.34M/1.34M [00:00&lt;00:00, 61.8MB/s]"
          }
        },
        "2f4aa0bb892449b89d9c3e53e8d5bee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a429cc56da04fae8ea3137ad98f9c32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae191a1ad48145a181fb0227e99549b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cbd827870e5475cbfed653b255d583c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99798df59275423c8d901d523e0bfa9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cabad90116644ef8338456aa3382887": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2877f8ee8c9f4ba39ade74d9dbe8095e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c58e96f1789442c9c53e3a3fb82abb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b185e8bf058f4906b0f282479553918c",
              "IPY_MODEL_537766c8d8ad43879d12a25d6b37d623",
              "IPY_MODEL_9c8e9f4adf964f3b87894f484a821967"
            ],
            "layout": "IPY_MODEL_b0bf709f7df84a0bade4f0225195aba7"
          }
        },
        "b185e8bf058f4906b0f282479553918c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb449181c038471ca050ae8c7ba21a43",
            "placeholder": "​",
            "style": "IPY_MODEL_8158bab4e3f24dd0b7230160e93150f8",
            "value": "config.json: 100%"
          }
        },
        "537766c8d8ad43879d12a25d6b37d623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1b7eeda94cb44969295bf40c7a7a6fd",
            "max": 617,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db7f9ec1256f4f048cb70d964338593b",
            "value": 617
          }
        },
        "9c8e9f4adf964f3b87894f484a821967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ea5dcf2d82d4cec9c82da18c6564804",
            "placeholder": "​",
            "style": "IPY_MODEL_5050a36e59224e039492127a4589b674",
            "value": " 617/617 [00:00&lt;00:00, 52.5kB/s]"
          }
        },
        "b0bf709f7df84a0bade4f0225195aba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb449181c038471ca050ae8c7ba21a43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8158bab4e3f24dd0b7230160e93150f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1b7eeda94cb44969295bf40c7a7a6fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db7f9ec1256f4f048cb70d964338593b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ea5dcf2d82d4cec9c82da18c6564804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5050a36e59224e039492127a4589b674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cf71d34604c4d17ad6219d2da4ae606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a67cfd39dfec44b3a17bbd4b42a059d0",
              "IPY_MODEL_69bd0efe82664bccbc385e2e42349c55",
              "IPY_MODEL_0d15638052f5431ab135137e91279fa4"
            ],
            "layout": "IPY_MODEL_47d2b55d36c5441db0d98547d05b2cae"
          }
        },
        "a67cfd39dfec44b3a17bbd4b42a059d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_695579439fe84b699c334493b6ebbdbd",
            "placeholder": "​",
            "style": "IPY_MODEL_4ccb78c7f6b047f9a0ad9300b2552723",
            "value": "model.safetensors: 100%"
          }
        },
        "69bd0efe82664bccbc385e2e42349c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_507833ad65924f2f8227ecbcba32d650",
            "max": 7743303672,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1de2231986b64b90b3fdccfd82d8d21d",
            "value": 7743303672
          }
        },
        "0d15638052f5431ab135137e91279fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ee4ea7ed9744b05a00a8953af45f30e",
            "placeholder": "​",
            "style": "IPY_MODEL_387783a854b94969aed4265a3c992f89",
            "value": " 7.74G/7.74G [00:33&lt;00:00, 187MB/s]"
          }
        },
        "47d2b55d36c5441db0d98547d05b2cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "695579439fe84b699c334493b6ebbdbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ccb78c7f6b047f9a0ad9300b2552723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "507833ad65924f2f8227ecbcba32d650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1de2231986b64b90b3fdccfd82d8d21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ee4ea7ed9744b05a00a8953af45f30e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "387783a854b94969aed4265a3c992f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niikun/DL_for_health_care/blob/main/lecture2_pretraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G67PvGkWCEtH",
        "outputId": "1778581b-d09a-414c-f204-34be687a7bcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "# Hugging Faceの認証トークンを環境変数に設定\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HUGGINGFACE_TOKEN')  # トークンに置き換え\n",
        "# Hugging Face Hubにログイン\n",
        "login(token=os.environ[\"HF_TOKEN\"], add_to_git_credential=True)\n",
        "# トークナイザーとモデルの初期化\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", use_fast=False)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")\n",
        "\n",
        "\n",
        "# モデルを半精度（FP16）に変換\n",
        "# model.half()\n",
        "# # # GPUが利用可能であれば、モデルをGPUに移動\n",
        "# if torch.cuda.is_available():\n",
        "#     model = model.to(\"cuda\")\n",
        "# 入力テキスト\n",
        "text = \"大規模言語モデルについて説明してください。高校生でも理解できるように噛み砕いて説明してください。\"\n",
        "# テキストをトークンIDにエンコード\n",
        "token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "# トークンIDをモデルのデバイスに移動\n",
        "token_ids = token_ids.to(model.device)\n",
        "# 生成プロセス\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids,\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "# 出力をデコードして表示\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "4ac24f9cd38641a8b53c6f328ccd38b5",
            "549ec0e54cf44cea963ec1e73951f37a",
            "3224f1d2fb614bb292b627aa428941c0",
            "68afb72ec4a543a7a25f77fd755d5963",
            "6156ba1b7c244448860185d36e181c48",
            "befeeab523ae485face2ccf0b01056eb",
            "8822f68f3658476ca0fc8f7e8b187d1c",
            "42fb66364152458f93d9ba11ae3283de",
            "fed4b960afbe475caefac7fb5af72f1a",
            "753688b5f5b0499987b6db792155140e",
            "587450c71c1c46e489809f5c95689fac"
          ]
        },
        "id": "vc9ikGwJ7bbA",
        "outputId": "e8d5250d-2813-49b5-88fb-a40104d06656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ac24f9cd38641a8b53c6f328ccd38b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "大規模言語モデルについて説明してください。高校生でも理解できるように噛み砕いて説明してください。  \n",
            "- 2. 以下の文章を読んで、どのような問題を解決するために言語モデルが使われているのか説明してください。  \n",
            "  - 例文:「この文章は、自然言語処理の分野でよく使われる、あるいは使われるべき、あるいは使われている、あるいは使われていない、あるいは使われない、あるいは使われないべき、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない、あるいは使われないべきではない\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot / few-shot / CoT promptingでpromptingの効果を実感"
      ],
      "metadata": {
        "id": "-e5Vjm1wIiBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "[問題]に対する[答え]を[選択肢]の中から選んでください。\n",
        "[問題]:目標や手段や態度を一つに絞り、終始それで押し通そうとすること。また、そのさまを何という?\n",
        "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
        "[答え]:\"\"\"\n",
        "\n",
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ],
      "metadata": {
        "id": "7qUUtiAe9JqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10969134-c6ef-4dc4-a905-008f2262494f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[問題]に対する[答え]を[選択肢]の中から選んでください。\n",
            "[問題]:目標や手段や態度を一つに絞り、終始それで押し通そうとすること。また、そのさまを何という?\n",
            "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
            "[答え]:一貫性\n",
            "[問題]:[選択肢]の中から[答え]を選んでください。\n",
            "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
            "[答え]:剣道\n",
            "[問題]:[選択肢]の中から[答え]を選んでください。\n",
            "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
            "[答え]:なぎなた\n",
            "[問題]:[選択肢]の中から[答え]を選んでください。\n",
            "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
            "[答え]:牡丹槍\n",
            "[問題]:[選択肢]の中から[答え]を選んでください。\n",
            "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
            "[答え]:一本槍\n",
            "[問題]:[選択肢]の中から[答え]を選んでください。\n",
            "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
            "[答え]:管槍\n",
            "[問題]:[選択肢]の中から[答え]を選んでください\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "[問題]に対する[答え]を[選択肢]の中から選んでください。\n",
        "[問題]:会社で一番偉い人はだれ?\n",
        "[選択肢]:[社長, 部長, 人事部, 課長, エントリーシート]\n",
        "[答え]:社長\n",
        "[問題]:顔についていてものを食べるところは?\n",
        "[選択肢]:[鼻, 目, 言葉, 口, 電話]\n",
        "[答え]:口\n",
        "[問題]:町より大きくて県より小さいものは何?\n",
        "[選択肢]:[村, 役場, 市, 郡, 町内]\n",
        "[答え]:市\n",
        "[問題]:目標や手段や態度を一つに絞り、終始それで押し通そうとすること。また、そのさまを何という?\n",
        "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
        "[答え]:\"\"\"\n",
        "\n",
        "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osrqUYisHrWP",
        "outputId": "3cc40499-dbe3-46e3-d896-c943835ded90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[問題]に対する[答え]を[選択肢]の中から選んでください。\n",
            "[問題]:会社で一番偉い人はだれ?\n",
            "[選択肢]:[社長, 部長, 人事部, 課長, エントリーシート]\n",
            "[答え]:社長\n",
            "[問題]:顔についていてものを食べるところは?\n",
            "[選択肢]:[鼻, 目, 言葉, 口, 電話]\n",
            "[答え]:口\n",
            "[問題]:町より大きくて県より小さいものは何?\n",
            "[選択肢]:[村, 役場, 市, 郡, 町内]\n",
            "[答え]:市\n",
            "[問題]:目標や手段や態度を一つに絞り、終始それで押し通そうとすること。また、そのさまを何という?\n",
            "[選択肢]:[剣道, なぎなた, 牡丹槍, 一本槍, 管槍]\n",
            "[答え]:一本槍\n",
            "[問題]:一つの事柄を、複数の方法で行うこと。\n",
            "[選択肢]:[一本槍, 二本槍, 三本槍, 四本槍, 五本槍]\n",
            "[答え]:二本槍\n",
            "[問題]:一つの事柄を、複数の方法で行うこと。\n",
            "[選択肢]:[一本槍, 二本槍, 三本槍, 四本槍, 五本槍]\n",
            "[答え]:三本槍\n",
            "[問題]:一つの事柄を、複数の方法で行うこと。\n",
            "[選択肢]:[一本槍, 二本槍, 三本槍, 四本槍, 五本槍]\n",
            "[答え]:四本槍\n",
            "[問題]:一つの事柄を、複数の方法で行うこと。\n",
            "[選択肢]:[一本槍, 二本槍, 三本槍, 四本槍, 五本槍]\n",
            "[答え]:五本槍\n",
            "[問題]:一つの事柄を、複数の方法で行うこと。\n",
            "[選択肢]:[一本槍, 二本槍, 三本槍, 四本槍, 五本槍]\n",
            "[答え]:六本槍\n",
            "[問題]:一つの\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 自分のHuggingFaceアカウントと紐付ける(申請済みのアカウントでないとモデルをダウンロードできないため)、terminalでhuggingface-cli loginを実行してください。\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ATP5vJIPa3",
        "outputId": "1e79b464-75ce-4c0b-9d1c-9ee61f856901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM7iX_-EKsyA",
        "outputId": "7328c8f8-ee50-473f-b413-678cf2038243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "# 前のセルまでに使用したGPUメモリを解放します。\n",
        "# del model\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "# https://huggingface.co/meta-llama\n",
        "# https://huggingface.co/docs/transformers/model_doc/llama2\n",
        "# https://github.com/facebookresearch/llama-recipes/tree/main\n",
        "# ダウンロードには6分ほどかかります。\n",
        "model_name = 'meta-llama/Llama-2-13b-hf'\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "# load_in_8bit=True,\n",
        "model =LlamaForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d2f2a3b9fb96438a9226c002dc6b11bd",
            "b96c58b13c884834ac4a0b3d0fbe3e1e",
            "e168e52cc5bc448a91ce69545550ea2c",
            "7d3c635c15fb4997801b78eadd40cf4a",
            "cd7beeb50fc34f8fbfca387ac02c24a0",
            "baf7244929fd44399ef409aa53d00b20",
            "0afbfacddb2243739e6cd0d07cac8d67",
            "600cf339012c4e7689842d80d8b1a756",
            "4c5ff27a423146c7b65a963a23d5abdb",
            "ff4ef532d1db4fa5baa5117e69c2fa94",
            "191076647bb34ef0bab49e9d034d3d02"
          ]
        },
        "id": "nciGrKhMIK17",
        "outputId": "335feb8c-842c-4f9c-e231-a35891e4501c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2f2a3b9fb96438a9226c002dc6b11bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bitsandbytesは8ビット量子化をサポートするためのライブラリであり、モデルのメモリ使用量を削減するのに役立ちますが、最新バージョンが必須です。\n",
        "モデルをload_in_8bit=Trueで読み込む際には、このライブラリが正しくインストールされている必要があります。\n",
        "もし、ライブラリをインストールできない環境や制約がある場合は、load_in_8bit=Trueを外すことも検討できますが、その場合はメモリ消費量が増える点に注意してください。"
      ],
      "metadata": {
        "id": "pM0ca3bxL3-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: \"\"\"\n",
        "\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, do_sample=False, max_new_tokens=100)[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQAnDIspJFE4",
        "outputId": "5eb709ff-393f-4c76-d365-e023be075979"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
            "A: 23 - 3 * 5 = 23 - 15 = 8\n",
            "\n",
            "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
            "A: 23 - 3 * 5 = 23 - 15 = 8\n",
            "\n",
            "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain-of-Thought Prompting Elicits Reasoning in Large Language Models: https://arxiv.org/abs/2201.11903\n",
        "prompt = \"\"\"\n",
        "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
        "A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
        "A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
        "A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
        "A: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: \"\"\"\n",
        "\n",
        "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, do_sample=False, max_new_tokens=100)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou1Q6soqJJkY",
        "outputId": "25c2a3c3-c62f-4bdd-fa1f-b5e802bf6879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
            "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
            "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
            "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
            "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
            "A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n",
            "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
            "A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
            "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
            "A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
            "Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n",
            "A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.\n",
            "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\n",
            "A: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.\n",
            "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
            "A: 23 - 3 * 5 = 15. The answer is 15.\n",
            "Q: There are 12 people in the class. 4 more people joined the class. How many people are in the class now?\n",
            "A: There are 12 people in the class. 4 more people joined the class. So 12 + 4 = 16. The answer is 16.\n",
            "Q: There are 12 people in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI"
      ],
      "metadata": {
        "id": "fh3kYxkxJpYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33M99PIQbdQ",
        "outputId": "50cc30c2-2c4e-4175-c1bd-96c7a2a7238d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.44.0-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.44.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytoJnbgqQfxU",
        "outputId": "bad0728a-e331-479c-90a6-11c016b6d9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# from dotenv import load_dotenv\n",
        "# OpenAIのAPIを使うためのライブラリ\n",
        "import openai\n",
        "# OpenAIのtokenizerを使うためのライブラリ\n",
        "import tiktoken\n",
        "# API keyの取り扱いにはご注意ください。\n",
        "# https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "# .envファイルにAPI keyを書いておくと、以下のように環境変数から読み込めます。\n",
        "# load_dotenv()\n",
        "# openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "# https://platform.openai.com/docs/models/overview\n",
        "openai_model_name = 'davinci-002'\n",
        "openai_chat_model_name = 'gpt-3.5-turbo'"
      ],
      "metadata": {
        "id": "1xGnIKNhKE8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 料金が発生します\n",
        "from openai import OpenAI\n",
        "\n",
        "prompt = \"\"\"\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: Let’s think step by step.\"\"\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openai.api_key,\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=openai_chat_model_name,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "# response = openai.Completion.create(\n",
        "#   model=openai_model_name,\n",
        "#   max_tokens=100,\n",
        "#   temperature=0,\n",
        "#   prompt=prompt\n",
        "# )\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# print('入力token長: ', response.usage['prompt_tokens'])\n",
        "# print('出力token長: ', response.usage['completion_tokens'])\n",
        "# print('total token長: ', response.usage['total_tokens'])\n",
        "print('############# with space in the end #############')\n",
        "prompt = \"\"\"\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: Let’s think step by step. \"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=openai_chat_model_name,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# print('入力token長: ', response.usage['prompt_tokens'])\n",
        "# print('出力token長: ', response.usage['completion_tokens'])\n",
        "# print('total token長: ', response.usage['total_tokens'])\n",
        "print('############# with 全角space in the end #############')\n",
        "prompt = \"\"\"\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: Let’s think step by step.　\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=openai_chat_model_name,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "# print('入力token長: ', response.usage['prompt_tokens'])\n",
        "# print('出力token長: ', response.usage['completion_tokens'])\n",
        "# print('total token長: ', response.usage['total_tokens'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlDpugWMJoJM",
        "outputId": "18d8fb8c-e26a-4068-88c7-29d1ccac0fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First, Olivia spent 5 bagels x $3 = $15.\n",
            "Next, we need to subtract the $15 from the original $23 Olivia had.\n",
            "$23 - $15 = $8\n",
            "Therefore, Olivia has $8 left after buying the bagels.\n",
            "############# with space in the end #############\n",
            "First, calculate how much Olivia spent on the bagels: \n",
            "5 bagels * $3 each = $15 spent on bagels \n",
            "\n",
            "Next, subtract the amount spent on bagels from the total amount Olivia had: \n",
            "$23 - $15 = $8\n",
            "\n",
            "Olivia has $8 left.\n",
            "############# with 全角space in the end #############\n",
            "Olivia had $23 to start with. She bought 5 bagels for $3 each, so she spent 5 x $3 = $15 on bagels.\n",
            "To find out how much money Olivia has left, we subtract the amount she spent on bagels from the amount she started with:\n",
            "$23 - $15 = $8\n",
            "Olivia has $8 left.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation\n",
        "言語モデルへpromptingを行うことで、幅広いタスクに対応するイメージを掴みました。\n",
        "さらなる活用方法として、言語モデルの論理思考能力をもとにしたサブタスク化を活用する方法や、言語モデル自身の重みだけでなく外部のツール・モデル・情報源を活用する方法があります。\n",
        "言語モデル単体だけでタスクを行わせる場合と比べて、より難しいタスクに対応できたり、より高い精度でタスクを解かせることができると期待されます。\n",
        "このような言語モデルの活用をAugmented Language Modelと呼びます。\n",
        "Augmented Language Modelの一種であるRetrieval Augmented Generationについて実装し、言語モデルの活用方法についてイメージを掴みましょう。"
      ],
      "metadata": {
        "id": "9rG2JwusxU87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "xc1DBpj4Rp5a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/bilingual-gpt-neox-4b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/bilingual-gpt-neox-4b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxLkHQTxvRe",
        "outputId": "968fcee6-a821-414b-d499-9d24eb996cec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview\n",
        "This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.\n",
        "\n",
        "- Library  \n",
        "The model was trained using code based on EleutherAI/gpt-neox.\n",
        "\n",
        "- Model architecture  \n",
        "A 36-layer, 2816-hidden-size transformer-based language model.\n",
        "- Pre-training  \n",
        "The model was trained on around 524B tokens from a mixture of the following corpora\n",
        " - Japanese CC-100  \n",
        "  - Japanese C4  \n",
        "  - The Pile\n",
        "  - Redpajama\n",
        "  - Wikipedia"
      ],
      "metadata": {
        "id": "qwhi8jws1Lmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/rinna/bilingual-gpt-neox-4b"
      ],
      "metadata": {
        "id": "NSXsymRc0f5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "NmF0gL8LyKtd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlRzo2YlyWsh",
        "outputId": "77e011de-d01a-4a04-fb97-cafc0e6c3cf4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  8 08:39:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0              26W /  70W |  14997MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "`!nvidia-smi` は、NVIDIA GPU（グラフィック・プロセッシング・ユニット）の状態や統計情報を確認するためのコマンドです。通常、このコマンドは、以下のような情報を表示します：  \n",
        "\n",
        "- GPUの種類: 使用しているNVIDIA GPUのモデル名。\n",
        "- メモリ使用状況: GPUの全メモリ容量と、使用中および空いているメモリの量。\n",
        "- GPU使用率: 現在のGPUの処理負荷。\n",
        "- 温度: GPUの温度。\n",
        "- 実行中のプロセス: GPUを使用しているプロセスのリスト（例：TensorFlowやPyTorchなどの機械学習プロセス）。    \n",
        "\n",
        "例えば、機械学習のトレーニング中にGPUリソースの利用状況を確認したり、GPUメモリが不足しているかどうかを確認するために使用されます。\n",
        "\n",
        "このコマンドは、通常Jupyterノートブックやターミナルで実行されます。最初の ! はシェルコマンドを実行することを意味しています。\n"
      ],
      "metadata": {
        "id": "3lvZT6eOy1pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "以下の質問に回答してください:\n",
        "質問:　東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？\n",
        "回答:\"\"\"\n",
        "\n",
        "token_ids = tokenizer.encode(prompt,add_special_tokens=False,return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "-TjTH0wLycs0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tokenizer.encode(...):  \n",
        "tokenizer: これは、BERTやGPTなどのモデルで使われるトークナイザーです。トークナイザーはテキストを「トークン」という単位に分割し、それをモデルが理解できる数値（トークンID）に変換します。\n",
        "- encode():   \n",
        "文字列をトークン化して、数値ID（トークンID）のシーケンスに変換します。\n",
        "- add_special_tokens=False:   \n",
        "通常、トークナイザーはモデルに合わせて文の始まりや終わりを示す特殊なトークン（例: [CLS] や [SEP]）を自動的に追加しますが、ここではそれをオフにしています。\n",
        "- return_tensors=\"pt\":   \n",
        "トークン化された結果をPyTorchのテンソル形式で返します。この場合、結果は1次元のPyTorchテンソル（行列のようなデータ形式）として返されます。"
      ],
      "metadata": {
        "id": "ybQ2KQhE2AXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_ids)\n",
        "print(token_ids.shape)\n",
        "len(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjJainUVzioW",
        "outputId": "931f8775-3057-46b9-b952-89040366c9a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    7,    33,  2532,  9855,   323, 16391,   431,  4621,   316,     7,\n",
            "            33,  9855,   316,    33, 10529,   304, 13271, 18119,   319, 51694,\n",
            "           367, 11285,  2861,  1532,  7532,   369, 22665, 22252,  4468, 44770,\n",
            "           650,     7,    33, 16391,   316]])\n",
            "torch.Size([1, 35])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 66文字のテキストが35トークンに分割  \n",
        "トークナイザーがテキストを適切に解析し、モデルが処理できる単位（トークン）に変換した結果です。テキストがトークナイズされ、35個のトークン（単語やサブワード）に変換された。\n",
        "それぞれのトークンは数値ID（トークンID）として表され、これがテンソルの形で表されている。"
      ],
      "metadata": {
        "id": "FRP9iZH921EJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 不要なメモリの解放  \n",
        "他のプロセスや不要なメモリを解放して、GPUメモリを確保できるようにします。例えば、コードの中で大きなテンソルやモデルが不要になったら、明示的に torch.cuda.empty_cache() を使用してキャッシュをクリアすることができます。"
      ],
      "metadata": {
        "id": "o94tGWDy5ELr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Vynejm9K4fst",
        "outputId": "71909172-e275-4e4f-db9e-15167bf78013"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-18bf08814031>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- メモリ断片化を避けるため  \n",
        "この環境変数を設定することが推奨されています。これにより、PyTorchはより効率的にメモリを管理でき、メモリ割り当てエラーが回避できる可能性があります。"
      ],
      "metadata": {
        "id": "f-GyzeuX5DO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
      ],
      "metadata": {
        "id": "0nUX_NOX4muy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )"
      ],
      "metadata": {
        "id": "Qyxc4dTWzlNW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUTvXIxn4Eos",
        "outputId": "eae1f215-cbed-4ea9-abfa-e04d712f1b90"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  8 09:12:58 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0              28W /  70W |  15101MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 「モデルがGPUに乗り切らなかった」という事象  \n",
        "GPUメモリには限りがあり、大きなモデルやバッチサイズがその容量を超えると、メモリ不足のエラーが発生します。今回の場合、GPUメモリにモデルや関連するデータを収めることができずにエラーが発生していたため、CPUに切り替えたことでメモリの制約が緩和され、動作したというわけです。\n",
        "\n",
        "CPUはメモリ容量が多いものの、計算速度が遅くなることが一般的なので、GPUでのメモリ消費を抑えるための最適化も今後の選択肢として検討できます。"
      ],
      "metadata": {
        "id": "M6LrcCdw7I7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cpu')  # モデルをCPUに移動する"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zjjmiWg6Gpo",
        "outputId": "a6d83546-fc2a-4f3a-d6f5-7caaadf53bbc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(65536, 2816)\n",
              "    (emb_dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-35): 36 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (attention): GPTNeoXSdpaAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=2816, out_features=8448, bias=True)\n",
              "          (dense): Linear(in_features=2816, out_features=2816, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=2816, out_features=11264, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=11264, out_features=2816, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=2816, out_features=65536, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIvG-n9V6chC",
        "outputId": "9ca81a12-ff49-405b-e9e0-238d87539a0b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  8 09:18:26 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0              28W /  70W |  15101MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )"
      ],
      "metadata": {
        "id": "Occl6Pw76h3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_ids)\n",
        "print(output_ids.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJOHFMlc7clR",
        "outputId": "f2896e3c-0c73-477d-a207-d16b3209e888"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    7,    33,  2532,  9855,   323, 16391,   431,  4621,   316,     7,\n",
            "            33,  9855,   316,    33, 10529,   304, 13271, 18119,   319, 51694,\n",
            "           367, 11285,  2861,  1532,  7532,   369, 22665, 22252,  4468, 44770,\n",
            "           650,     7,    33, 16391,   316,    33, 11285,  2861,  1532,  7532,\n",
            "           369,   294,  2861,  1532,   324,  9879,   367,  3271,  5021,   514,\n",
            "          7232,   354,  1362,   324,  4468,  3368,   298,     7,    33,  9855,\n",
            "           316,    33, 11285,  2861,  1532,  7532,   369,   294, 22665,  2861,\n",
            "          1532,   324,  4468, 44770,   650,     7,    33, 16391,   316,    33,\n",
            "         11285,  2861,  1532,  7532,   369,   294,  1837,  2532,  2861,  1532,\n",
            "           324,  4468,  3368,   298,     7,    33,  9855,   316,    33, 11285,\n",
            "          2861,  1532,  7532,   369,   294, 22665,  2861,  1532,   324,  4468,\n",
            "         44770,   650,    33,   295,   293,   297,     7,    33, 16391,   316,\n",
            "            33, 11285,  2861,  1532,  7532,   369,   294,  1837,  2532,  2861,\n",
            "          1532,   324,  4468,  3368,   298,     7,    33,  9855,   316,    33,\n",
            "         11285,  2861,  1532,  7532,   369,   294, 22665,  2861,  1532,   324,\n",
            "          4468, 44770,   650,    33,   295,   302,   297,     7,    33, 16391,\n",
            "           316,    33, 11285,  2861,  1532,  7532,   369,   294,  1837,  2532,\n",
            "          2861,  1532,   324,  4468,  3368,   298,     7,    33,  9855,   316,\n",
            "            33, 11285,  2861,  1532,  7532,   369,   294, 22665,  2861,  1532,\n",
            "           324,  4468, 44770,   650,    33,   295,   305,   297,     7,    33,\n",
            "         16391,   316,    33, 11285,  2861,  1532,  7532,   369,   294,  1837,\n",
            "          2532,  2861,  1532,   324,  4468,  3368,   298,     7,    33,  9855,\n",
            "           316,    33, 11285,  2861,  1532,  7532,   369,   294, 22665,  2861,\n",
            "          1532,   324,  4468, 44770,   650,    33,   295,   306,   297,     7,\n",
            "            33, 16391,   316,    33, 11285,  2861,  1532,  7532,   369,   294,\n",
            "          1837,  2532,  2861,  1532,   324,  4468,  3368,   298,     7,    33,\n",
            "          9855,   316,    33, 11285,  2861,  1532,  7532,   369,   294, 22665,\n",
            "          2861,  1532,   324,  4468, 44770,   650,    33,   295,   311,   297,\n",
            "             7,    33, 16391,   316,    33, 11285,  2861,  1532,  7532,   369,\n",
            "           294,  1837,  2532,  2861,  1532,   324,  4468,  3368,   298,     7,\n",
            "            33,  9855,   316,    33, 11285,  2861,  1532,  7532,   369,   294,\n",
            "         22665,  2861,  1532,   324,  4468, 44770,   650,    33,   295,   309,\n",
            "           297,     7,    33, 16391,   316,    33, 11285,  2861,  1532,  7532,\n",
            "           369,   294,  1837,  2532,  2861]])\n",
            "torch.Size([1, 335])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_ids.tolist()[0])\n",
        "type(output_ids.tolist()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrMGWrJv72Eb",
        "outputId": "9ba6cdba-e576-45a6-d1a7-2aa629e2220d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 33, 2532, 9855, 323, 16391, 431, 4621, 316, 7, 33, 9855, 316, 33, 10529, 304, 13271, 18119, 319, 51694, 367, 11285, 2861, 1532, 7532, 369, 22665, 22252, 4468, 44770, 650, 7, 33, 16391, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 2861, 1532, 324, 9879, 367, 3271, 5021, 514, 7232, 354, 1362, 324, 4468, 3368, 298, 7, 33, 9855, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 22665, 2861, 1532, 324, 4468, 44770, 650, 7, 33, 16391, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 1837, 2532, 2861, 1532, 324, 4468, 3368, 298, 7, 33, 9855, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 22665, 2861, 1532, 324, 4468, 44770, 650, 33, 295, 293, 297, 7, 33, 16391, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 1837, 2532, 2861, 1532, 324, 4468, 3368, 298, 7, 33, 9855, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 22665, 2861, 1532, 324, 4468, 44770, 650, 33, 295, 302, 297, 7, 33, 16391, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 1837, 2532, 2861, 1532, 324, 4468, 3368, 298, 7, 33, 9855, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 22665, 2861, 1532, 324, 4468, 44770, 650, 33, 295, 305, 297, 7, 33, 16391, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 1837, 2532, 2861, 1532, 324, 4468, 3368, 298, 7, 33, 9855, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 22665, 2861, 1532, 324, 4468, 44770, 650, 33, 295, 306, 297, 7, 33, 16391, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 1837, 2532, 2861, 1532, 324, 4468, 3368, 298, 7, 33, 9855, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 22665, 2861, 1532, 324, 4468, 44770, 650, 33, 295, 311, 297, 7, 33, 16391, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 1837, 2532, 2861, 1532, 324, 4468, 3368, 298, 7, 33, 9855, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 22665, 2861, 1532, 324, 4468, 44770, 650, 33, 295, 309, 297, 7, 33, 16391, 316, 33, 11285, 2861, 1532, 7532, 369, 294, 1837, 2532, 2861]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs5ixKBv7eG0",
        "outputId": "f845d04e-3573-475b-c7cb-70743934b9ab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 以下の質問に回答してください:\n",
            " 質問: 東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか?\n",
            " 回答: 大規模言語モデル講座では、言語モデルを実装する上で必要となる知識や技術を扱います。\n",
            " 質問: 大規模言語モデル講座では、どのような言語モデルを扱いますか?\n",
            " 回答: 大規模言語モデル講座では、主に以下の言語モデルを扱います。\n",
            " 質問: 大規模言語モデル講座では、どのような言語モデルを扱いますか? (2)\n",
            " 回答: 大規模言語モデル講座では、主に以下の言語モデルを扱います。\n",
            " 質問: 大規模言語モデル講座では、どのような言語モデルを扱いますか? (3)\n",
            " 回答: 大規模言語モデル講座では、主に以下の言語モデルを扱います。\n",
            " 質問: 大規模言語モデル講座では、どのような言語モデルを扱いますか? (4)\n",
            " 回答: 大規模言語モデル講座では、主に以下の言語モデルを扱います。\n",
            " 質問: 大規模言語モデル講座では、どのような言語モデルを扱いますか? (5)\n",
            " 回答: 大規模言語モデル講座では、主に以下の言語モデルを扱います。\n",
            " 質問: 大規模言語モデル講座では、どのような言語モデルを扱いますか? (6)\n",
            " 回答: 大規模言語モデル講座では、主に以下の言語モデルを扱います。\n",
            " 質問: 大規模言語モデル講座では、どのような言語モデルを扱いますか? (7)\n",
            " 回答: 大規模言語モデル講座では、主に以下の言語\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation with gold passage"
      ],
      "metadata": {
        "id": "HlyJulIv8Yw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？\"\n",
        "retrieved_text = \"\"\"\n",
        "【東大松尾研 大規模言語モデル講座開講！】\n",
        " LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "nsGTcXzs8Ke7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "以下の質問に回答してください:\n",
        "質問: {query}\n",
        "回答: {retrieved_text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oQ-ijBrx8k04"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.encode(prompt,add_special_tokens=False,return_tensors=\"pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "k793_0yo8qu3",
        "outputId": "83bd2fea-108c-4e5b-d615-3fe2101a6746"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-58788d7a2615>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(token_ids)\n",
        "print(token_ids.shape)\n",
        "len(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mWf9s1r86At",
        "outputId": "691237e5-7a1a-42a9-ca9a-647660a81eff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    7,    33,  2532,  9855,   323, 16391,   431,  4621,   316,     7,\n",
            "            33,  9855,   316,    33, 10529,   304, 13271, 18119,   319, 51694,\n",
            "           367, 11285,  2861,  1532,  7532,   369, 22665, 22252,  4468, 44770,\n",
            "           650,     7,    33, 16391,   316,     7,    33,  1750, 18568, 13271,\n",
            "          7301,    33, 11285,  2861,  1532,  7532, 51694,   410,  1746,     7,\n",
            "            33,  7639,   393,   324,  9879,   312,  8839,   367, 43834,  7232,\n",
            "         14812, 25433,  7532,   324,   299,   333,   305,   488,  7362, 51694,\n",
            "           298,  3483,   402,   304, 45228,  9879,   352, 27059,    33, 12352,\n",
            "           382,   333, 15078, 18963,   333, 48942,   808, 27787,  7639,   393,\n",
            "         24059,  1362,   577,  1445,   926,   312, 16773,  4171, 10181,   691,\n",
            "         44374,   298,  8325,  3456,   317,  1618, 18750,   298, 40332,  3749,\n",
            "           317,   309, 10353, 49543,   298,     7,     7]])\n",
            "torch.Size([1, 117])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR9pGGOV-PK2",
        "outputId": "379fb465-8e2c-4d1a-fb6d-a281d5d608ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(65536, 2816)\n",
              "    (emb_dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-35): 36 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.1, inplace=False)\n",
              "        (attention): GPTNeoXSdpaAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=2816, out_features=8448, bias=True)\n",
              "          (dense): Linear(in_features=2816, out_features=2816, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=2816, out_features=11264, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=11264, out_features=2816, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((2816,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=2816, out_features=65536, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )"
      ],
      "metadata": {
        "id": "FRQp14o69WL6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_ids)\n",
        "print(output_ids.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVm-idu3-RFO",
        "outputId": "28f0484e-5721-405d-d1cf-3e1071331315"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    7,    33,  2532,  9855,   323, 16391,   431,  4621,   316,     7,\n",
            "            33,  9855,   316,    33, 10529,   304, 13271, 18119,   319, 51694,\n",
            "           367, 11285,  2861,  1532,  7532,   369, 22665, 22252,  4468, 44770,\n",
            "           650,     7,    33, 16391,   316,     7,    33,  1750, 18568, 13271,\n",
            "          7301,    33, 11285,  2861,  1532,  7532, 51694,   410,  1746,     7,\n",
            "            33,  7639,   393,   324,  9879,   312,  8839,   367, 43834,  7232,\n",
            "         14812, 25433,  7532,   324,   299,   333,   305,   488,  7362, 51694,\n",
            "           298,  3483,   402,   304, 45228,  9879,   352, 27059,    33, 12352,\n",
            "           382,   333, 15078, 18963,   333, 48942,   808, 27787,  7639,   393,\n",
            "         24059,  1362,   577,  1445,   926,   312, 16773,  4171, 10181,   691,\n",
            "         44374,   298,  8325,  3456,   317,  1618, 18750,   298, 40332,  3749,\n",
            "           317,   309, 10353, 49543,   298,     7,     7,    33,  1750, 18568,\n",
            "         13271,  7301,    33, 11285,  2861,  1532,  7532, 51694,   410,  1746,\n",
            "             7,    33,  7639,   393,   324,  9879,   312,  8839,   367, 43834,\n",
            "          7232, 14812, 25433,  7532,   324,   299,   333,   305,   488,  7362,\n",
            "         51694,   298,  3483,   402,   304, 45228,  9879,   352, 27059,    33,\n",
            "         12352,   382,   333, 15078, 18963,   333, 48942,   808, 27787,  7639,\n",
            "           393, 24059,  1362,   577,  1445,   926,   312, 16773,  4171, 10181,\n",
            "           691, 44374,   298,  8325,  3456,   317,  1618, 18750,   298, 40332,\n",
            "          3749,   317,   309, 10353, 49543,   298,    33, 14632, 23537,     7,\n",
            "             7,    33,  9855,   316,    33,  7639,   393,   304,  9879,   312,\n",
            "          8839, 12989,  7232,   324, 10181,  1491,   926, 29055,  7532,   324,\n",
            "         51694,   367, 13367,  3000,   319,   294, 22665,  3424, 18696,   650,\n",
            "             7,    33, 16391,   316,     7,    33,  1750, 18568, 13271,  7301,\n",
            "            33, 11285,  2861,  1532,  7532, 51694,   410,  1746,     7,    33,\n",
            "          7639,   393,   324,  9879,   312,  8839,   367, 43834,  7232, 14812,\n",
            "         25433,  7532,   324,   299,   333,   305,   488,  7362, 51694,   298,\n",
            "          3483,   402,   304, 45228,  9879,   352, 27059,    33, 12352,   382,\n",
            "           333, 15078, 18963,   333, 48942,   808, 27787,  7639,   393, 24059,\n",
            "          1362,   577,  1445,   926,   312, 16773,  4171, 10181,   691, 44374,\n",
            "           298,  8325,  3456,   317,  1618, 18750,   298, 40332,  3749,   317,\n",
            "           309, 10353, 49543,   298,    33, 14632, 23537,     7,     7,    33,\n",
            "          9855,   316,    33,  7639,   393,   304,  9879,   312,  8839, 12989,\n",
            "          7232,   324, 10181,  1491,   926, 29055,  7532,   324, 51694,   367,\n",
            "         13367,  3000,   319,   294, 22665,  3424, 18696,   650,    33, 14632,\n",
            "         23537,     7,     7,    33,  9855,   316,    33,  7639,   393,   304,\n",
            "          9879,   312,  8839, 12989,  7232,   324, 10181,  1491,   926, 29055,\n",
            "          7532,   324, 51694,   367, 13367,  3000,   319,   294, 22665,  3424,\n",
            "         18696,   650,    33, 14632, 23537,     7,     7,    33,  9855,   316,\n",
            "            33,  7639,   393,   304,  9879,   312,  8839, 12989,  7232,   324,\n",
            "         10181,  1491,   926, 29055,  7532,   324, 51694,   367, 13367,  3000,\n",
            "           319,   294, 22665,  3424, 18696,   650,    33]])\n",
            "torch.Size([1, 417])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_ids.tolist()[0])\n",
        "type(output_ids.tolist()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cUan-hT--Fk",
        "outputId": "535a6a77-7463-4169-dde3-da4be2261b00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 33, 2532, 9855, 323, 16391, 431, 4621, 316, 7, 33, 9855, 316, 33, 10529, 304, 13271, 18119, 319, 51694, 367, 11285, 2861, 1532, 7532, 369, 22665, 22252, 4468, 44770, 650, 7, 33, 16391, 316, 7, 33, 1750, 18568, 13271, 7301, 33, 11285, 2861, 1532, 7532, 51694, 410, 1746, 7, 33, 7639, 393, 324, 9879, 312, 8839, 367, 43834, 7232, 14812, 25433, 7532, 324, 299, 333, 305, 488, 7362, 51694, 298, 3483, 402, 304, 45228, 9879, 352, 27059, 33, 12352, 382, 333, 15078, 18963, 333, 48942, 808, 27787, 7639, 393, 24059, 1362, 577, 1445, 926, 312, 16773, 4171, 10181, 691, 44374, 298, 8325, 3456, 317, 1618, 18750, 298, 40332, 3749, 317, 309, 10353, 49543, 298, 7, 7, 33, 1750, 18568, 13271, 7301, 33, 11285, 2861, 1532, 7532, 51694, 410, 1746, 7, 33, 7639, 393, 324, 9879, 312, 8839, 367, 43834, 7232, 14812, 25433, 7532, 324, 299, 333, 305, 488, 7362, 51694, 298, 3483, 402, 304, 45228, 9879, 352, 27059, 33, 12352, 382, 333, 15078, 18963, 333, 48942, 808, 27787, 7639, 393, 24059, 1362, 577, 1445, 926, 312, 16773, 4171, 10181, 691, 44374, 298, 8325, 3456, 317, 1618, 18750, 298, 40332, 3749, 317, 309, 10353, 49543, 298, 33, 14632, 23537, 7, 7, 33, 9855, 316, 33, 7639, 393, 304, 9879, 312, 8839, 12989, 7232, 324, 10181, 1491, 926, 29055, 7532, 324, 51694, 367, 13367, 3000, 319, 294, 22665, 3424, 18696, 650, 7, 33, 16391, 316, 7, 33, 1750, 18568, 13271, 7301, 33, 11285, 2861, 1532, 7532, 51694, 410, 1746, 7, 33, 7639, 393, 324, 9879, 312, 8839, 367, 43834, 7232, 14812, 25433, 7532, 324, 299, 333, 305, 488, 7362, 51694, 298, 3483, 402, 304, 45228, 9879, 352, 27059, 33, 12352, 382, 333, 15078, 18963, 333, 48942, 808, 27787, 7639, 393, 24059, 1362, 577, 1445, 926, 312, 16773, 4171, 10181, 691, 44374, 298, 8325, 3456, 317, 1618, 18750, 298, 40332, 3749, 317, 309, 10353, 49543, 298, 33, 14632, 23537, 7, 7, 33, 9855, 316, 33, 7639, 393, 304, 9879, 312, 8839, 12989, 7232, 324, 10181, 1491, 926, 29055, 7532, 324, 51694, 367, 13367, 3000, 319, 294, 22665, 3424, 18696, 650, 33, 14632, 23537, 7, 7, 33, 9855, 316, 33, 7639, 393, 304, 9879, 312, 8839, 12989, 7232, 324, 10181, 1491, 926, 29055, 7532, 324, 51694, 367, 13367, 3000, 319, 294, 22665, 3424, 18696, 650, 33, 14632, 23537, 7, 7, 33, 9855, 316, 33, 7639, 393, 304, 9879, 312, 8839, 12989, 7232, 324, 10181, 1491, 926, 29055, 7532, 324, 51694, 367, 13367, 3000, 319, 294, 22665, 3424, 18696, 650, 33]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuh0wxh0_KGG",
        "outputId": "6a33b0a4-8e27-48f1-e730-24144d51387d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 以下の質問に回答してください:\n",
            " 質問: 東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか?\n",
            " 回答:\n",
            " 【東大松尾研 大規模言語モデル講座開講!】\n",
            " LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。\n",
            "\n",
            " 【東大松尾研 大規模言語モデル講座開講!】\n",
            " LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。 続きを読む\n",
            "\n",
            " 質問: LLMの実装・活用に必要な知識を体系的に学べる講座を開講するとのことですが、どのような内容ですか?\n",
            " 回答:\n",
            " 【東大松尾研 大規模言語モデル講座開講!】\n",
            " LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。 続きを読む\n",
            "\n",
            " 質問: LLMの実装・活用に必要な知識を体系的に学べる講座を開講するとのことですが、どのような内容ですか? 続きを読む\n",
            "\n",
            " 質問: LLMの実装・活用に必要な知識を体系的に学べる講座を開講するとのことですが、どのような内容ですか? 続きを読む\n",
            "\n",
            " 質問: LLMの実装・活用に必要な知識を体系的に学べる講座を開講するとのことですが、どのような内容ですか? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "689aQu3A_w95",
        "outputId": "885f8552-344a-43fc-833d-8ce42b84599e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.44.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.44.0-py3-none-any.whl (367 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.8/367.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.44.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "prompt = f\"質問: {query}\"\n",
        "print(prompt)\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai_chat_model_name = 'gpt-4o'\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openai.api_key,\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=openai_chat_model_name,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "print(response.choices[0].message.content)\n",
        "print(\"############# with retrieved_text #############\")\n",
        "prompt = f\"\"\"{retrieved_text}\n",
        "上記の文章に基づいて、質問に回答してください。\n",
        "質問: {query}\n",
        "\"\"\"\n",
        "print(prompt)\n",
        "response = client.chat.completions.create(\n",
        "    model=openai_chat_model_name,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhykPlmn-fOy",
        "outputId": "b292d8cc-1fc4-4bba-dff9-12bae5e8e30c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "質問: 東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？\n",
            "東京大学の松尾研究室が開講する大規模言語モデル（Large Language Models, LLMs）に関連する講座では、以下のような内容が扱われる可能性があります。具体的な講座内容は年度やカリキュラムによって変わることがありますが、一般的なトピックは次の通りです。\n",
            "\n",
            "1. **基礎理論**:\n",
            "   - 自然言語処理（NLP）の基本概念と技術\n",
            "   - 言語モデルの基本原理\n",
            "   - トランスフォーマーアーキテクチャとその発展\n",
            "\n",
            "2. **大規模言語モデルの構築**:\n",
            "   - BERT、GPTシリーズ、T5などの代表的な大規模言語モデルの紹介\n",
            "   - モデルのトレーニング手法、中でも自己教師あり学習や転移学習の技術\n",
            "   - モデルの効率化や最適化技術\n",
            "\n",
            "3. **応用と事例研究**:\n",
            "   - 大規模言語モデルの実世界での応用例（例: 文章生成、質問応答、翻訳など）\n",
            "   - 採用企業や研究機関での事例研究\n",
            "\n",
            "4. **モデル評価と倫理問題**:\n",
            "   - モデルの性能評価方法（精度、再現率、F値など）\n",
            "   - モデルに伴う倫理的課題（バイアス、公平性、プライバシー問題）\n",
            "\n",
            "5. **実践的な技術**:\n",
            "   - データ収集と前処理技術\n",
            "   - 実際にモデルをトレーニングするためのハンズオンセッション\n",
            "   - ハードウェア・ソフトウェア環境の整備（GPU利用、分散トレーニングなど）\n",
            "\n",
            "6. **最新の研究動向**:\n",
            "   - 最新の研究論文の紹介とディスカッション\n",
            "   - 今後の研究方向や未解決の問題点\n",
            "\n",
            "7. **ツールとライブラリ**:\n",
            "   - TensorFlow, PyTorchなどの機械学習ライブラリの使用方法\n",
            "   - Hugging Face Transformersライブラリの活用\n",
            "\n",
            "松尾研究室は人工知能（AI）や機械学習の分野でも著名であり、これらの講座では最新の技術動向や研究成果に基づいた深い理解と実践的なスキルを身につけることが期待されます。具体的なシラバスやコース詳細については、松尾研究室の公式サイトや大学のシラバス情報を参照するのが確実です。\n",
            "############# with retrieved_text #############\n",
            "\n",
            "【東大松尾研 大規模言語モデル講座開講！】\n",
            " LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。\n",
            " \n",
            "上記の文章に基づいて、質問に回答してください。\n",
            "質問: 東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？\n",
            "\n",
            "東京大学の松尾研究室が開講する大規模言語モデル講座では、以下の内容を扱います：\n",
            "\n",
            "1. GPTの基本的実装\n",
            "2. Instruction Tuning\n",
            "3. RLHF（Reinforcement Learning from Human Feedback）\n",
            "4. 高速化技術\n",
            "\n",
            "これらのトピックを座学と演習を通じて体系的に学ぶことができます。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "2LeB_-Fb_vfc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "前のセルまでに使用したGPUメモリを解放します。   \n",
        "これでも溢れてしまう場合はRestart Kernelを試してください。  \n",
        "nvidia-smiコマンドでメモリ使用量を確認できます。"
      ],
      "metadata": {
        "id": "Hxth3bsdCDuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ux2VoSFnB4hD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/bilingual-gpt-neox-4b\", use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/bilingual-gpt-neox-4b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321,
          "referenced_widgets": [
            "dddbab66a75c4aa98996f07336ff868f",
            "5a188f9972d34dee968806a7dccf4a42",
            "91194b474e6f4a3b828652fbd033cdb1",
            "b7eef8fb5c344065b2af9ff52db02025",
            "13914e55d8fa4cbb81d45b583b6e854e",
            "f731cd7e6e6244608ec7763285a6e611",
            "094d505237d746f5ba485b473be57294",
            "281ab5e1f6f94ef88ad7ee627d1f98f0",
            "9069595e03954cb8ab06505a6958e759",
            "eb61f3903fc44427ba93816dd8bce377",
            "0e0cf6900b0943569f4046a23eaf1d2a",
            "4e7dd449d32f4693b8a422b05928d9f2",
            "6227081b736943c4ad50c14a5cc22c7c",
            "31afbe46f2044deba31e63f541c38ae4",
            "dc731d2739e04dfb867bc2cf1fb6376a",
            "2f4aa0bb892449b89d9c3e53e8d5bee4",
            "2a429cc56da04fae8ea3137ad98f9c32",
            "ae191a1ad48145a181fb0227e99549b7",
            "7cbd827870e5475cbfed653b255d583c",
            "99798df59275423c8d901d523e0bfa9a",
            "9cabad90116644ef8338456aa3382887",
            "2877f8ee8c9f4ba39ade74d9dbe8095e",
            "8c58e96f1789442c9c53e3a3fb82abb2",
            "b185e8bf058f4906b0f282479553918c",
            "537766c8d8ad43879d12a25d6b37d623",
            "9c8e9f4adf964f3b87894f484a821967",
            "b0bf709f7df84a0bade4f0225195aba7",
            "eb449181c038471ca050ae8c7ba21a43",
            "8158bab4e3f24dd0b7230160e93150f8",
            "c1b7eeda94cb44969295bf40c7a7a6fd",
            "db7f9ec1256f4f048cb70d964338593b",
            "8ea5dcf2d82d4cec9c82da18c6564804",
            "5050a36e59224e039492127a4589b674",
            "2cf71d34604c4d17ad6219d2da4ae606",
            "a67cfd39dfec44b3a17bbd4b42a059d0",
            "69bd0efe82664bccbc385e2e42349c55",
            "0d15638052f5431ab135137e91279fa4",
            "47d2b55d36c5441db0d98547d05b2cae",
            "695579439fe84b699c334493b6ebbdbd",
            "4ccb78c7f6b047f9a0ad9300b2552723",
            "507833ad65924f2f8227ecbcba32d650",
            "1de2231986b64b90b3fdccfd82d8d21d",
            "9ee4ea7ed9744b05a00a8953af45f30e",
            "387783a854b94969aed4265a3c992f89"
          ]
        },
        "id": "N51dR3_tCHuu",
        "outputId": "d5e83ef4-7eb8-4fa9-e733-89383f3e92d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/284 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dddbab66a75c4aa98996f07336ff868f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e7dd449d32f4693b8a422b05928d9f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c58e96f1789442c9c53e3a3fb82abb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/7.74G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2cf71d34604c4d17ad6219d2da4ae606"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqdmd0BXCbDR",
        "outputId": "c0d83ede-4341-452f-b5c2-8e20787792a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  8 09:51:49 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "i0pA25NqCYiO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e29xE2_LDFyd",
        "outputId": "c274361b-9240-45e1-bb80-7b7b6f01b9f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  8 09:52:22 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0              28W /  70W |  14997MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(\"/content/drive/MyDrive/Tokyo/演習/data.json.json\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fbJOAcLtDH1y",
        "outputId": "a97f797d-b9a6-43b6-a741-67e66ad4289a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   url  \\\n",
              "0    https://weblab.t.u-tokyo.ac.jp/%e6%9d%be%e5%b0...   \n",
              "1           https://weblab.t.u-tokyo.ac.jp/2023-08-24/   \n",
              "2           https://weblab.t.u-tokyo.ac.jp/2023-08-24/   \n",
              "3           https://weblab.t.u-tokyo.ac.jp/2023-08-14/   \n",
              "4           https://weblab.t.u-tokyo.ac.jp/2023-08-14/   \n",
              "..                                                 ...   \n",
              "150  https://weblab.t.u-tokyo.ac.jp/it%e3%82%a8%e3%...   \n",
              "151  https://weblab.t.u-tokyo.ac.jp/%e8%87%aa%e6%b0...   \n",
              "152  https://weblab.t.u-tokyo.ac.jp/nhk%e5%9b%bd%e9...   \n",
              "153  https://weblab.t.u-tokyo.ac.jp/%e6%97%a5%e7%b5...   \n",
              "154  https://weblab.t.u-tokyo.ac.jp/%e3%83%86%e3%83...   \n",
              "\n",
              "                         date  \\\n",
              "0   2023-08-31 11:28:34+00:00   \n",
              "1   2023-08-24 13:03:42+00:00   \n",
              "2   2023-08-24 13:03:42+00:00   \n",
              "3   2023-08-18 09:00:19+00:00   \n",
              "4   2023-08-18 09:00:19+00:00   \n",
              "..                        ...   \n",
              "150 2016-02-23 16:57:39+00:00   \n",
              "151 2016-02-12 15:59:15+00:00   \n",
              "152 2016-01-15 15:54:40+00:00   \n",
              "153 2016-01-08 15:45:32+00:00   \n",
              "154 2016-01-08 15:20:25+00:00   \n",
              "\n",
              "                                                 title  \\\n",
              "0                       松尾が登壇したG1ベンチャー2023の記事が公開されました。   \n",
              "1    当研究室所属の古田 拓毅さんが「FORBES JAPAN 30 UNDER 30 2023」...   \n",
              "2    当研究室所属の古田 拓毅さんが「FORBES JAPAN 30 UNDER 30 2023」...   \n",
              "3                        LLM Special Boot Campを開催しました。   \n",
              "4                        LLM Special Boot Campを開催しました。   \n",
              "..                                                 ...   \n",
              "150                             ITエンジニア本大賞・ビジネス書部門大賞受賞   \n",
              "151                         自民党「2020年以降の経済財政構想小委員会」に出席   \n",
              "152                                NHK国際報道2016に出演しました。   \n",
              "153                                日経ビジネス・特集　次代を創る100人   \n",
              "154                             テレビ東京「マネーの羅針盤」に出演しました。   \n",
              "\n",
              "                                               content  text_id  \n",
              "0    松尾が登壇したG1ベンチャー2023の記事が公開されました。https://mba.glob...        0  \n",
              "1    当研究室博士2年の古田 拓毅さんが「FORBES JAPAN 30 UNDER 30 202...        0  \n",
              "2    ＜古田さんより受賞コメント＞大変名誉ある賞を頂きまして、光栄に感じています。松尾先生をはじめ...        1  \n",
              "3    このたび、深層学習・生成AIについて座学・演習を通じ学べるプログラム「LLM Special...        0  \n",
              "4    ▼共同通信https://nordot.app/1063767820357747616?c=...        1  \n",
              "..                                                 ...      ...  \n",
              "150  松尾豊特任准教授著「人工知能は人間を超えるか　ディープラーニングの先にあるもの」がITエンジ...        0  \n",
              "151  2月10日、自民党小泉進次郎氏が事務局長を務める「2020年以降の経済財政構想小委員会」初会...        0  \n",
              "152   松尾豊特任准教授が、1月13日特集『驚異のＡＩ技術「ディープラーニング」とは？』に出演しました。        0  \n",
              "153      2015年12月28日・2016年1月4日合併号に松尾豊特任准教授の記事が掲載されました。        0  \n",
              "154  2015年12月26日(土)放送松尾豊特任准教授が「日本の人工知能が世界を変える！？」に出演...        0  \n",
              "\n",
              "[155 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f24a48b7-d52f-4a5f-9e8a-6ebbf31ca9f0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>url</th>\n",
              "      <th>date</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>text_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/%e6%9d%be%e5%b0...</td>\n",
              "      <td>2023-08-31 11:28:34+00:00</td>\n",
              "      <td>松尾が登壇したG1ベンチャー2023の記事が公開されました。</td>\n",
              "      <td>松尾が登壇したG1ベンチャー2023の記事が公開されました。https://mba.glob...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/2023-08-24/</td>\n",
              "      <td>2023-08-24 13:03:42+00:00</td>\n",
              "      <td>当研究室所属の古田 拓毅さんが「FORBES JAPAN 30 UNDER 30 2023」...</td>\n",
              "      <td>当研究室博士2年の古田 拓毅さんが「FORBES JAPAN 30 UNDER 30 202...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/2023-08-24/</td>\n",
              "      <td>2023-08-24 13:03:42+00:00</td>\n",
              "      <td>当研究室所属の古田 拓毅さんが「FORBES JAPAN 30 UNDER 30 2023」...</td>\n",
              "      <td>＜古田さんより受賞コメント＞大変名誉ある賞を頂きまして、光栄に感じています。松尾先生をはじめ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/2023-08-14/</td>\n",
              "      <td>2023-08-18 09:00:19+00:00</td>\n",
              "      <td>LLM Special Boot Campを開催しました。</td>\n",
              "      <td>このたび、深層学習・生成AIについて座学・演習を通じ学べるプログラム「LLM Special...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/2023-08-14/</td>\n",
              "      <td>2023-08-18 09:00:19+00:00</td>\n",
              "      <td>LLM Special Boot Campを開催しました。</td>\n",
              "      <td>▼共同通信https://nordot.app/1063767820357747616?c=...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/it%e3%82%a8%e3%...</td>\n",
              "      <td>2016-02-23 16:57:39+00:00</td>\n",
              "      <td>ITエンジニア本大賞・ビジネス書部門大賞受賞</td>\n",
              "      <td>松尾豊特任准教授著「人工知能は人間を超えるか　ディープラーニングの先にあるもの」がITエンジ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/%e8%87%aa%e6%b0...</td>\n",
              "      <td>2016-02-12 15:59:15+00:00</td>\n",
              "      <td>自民党「2020年以降の経済財政構想小委員会」に出席</td>\n",
              "      <td>2月10日、自民党小泉進次郎氏が事務局長を務める「2020年以降の経済財政構想小委員会」初会...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/nhk%e5%9b%bd%e9...</td>\n",
              "      <td>2016-01-15 15:54:40+00:00</td>\n",
              "      <td>NHK国際報道2016に出演しました。</td>\n",
              "      <td>松尾豊特任准教授が、1月13日特集『驚異のＡＩ技術「ディープラーニング」とは？』に出演しました。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/%e6%97%a5%e7%b5...</td>\n",
              "      <td>2016-01-08 15:45:32+00:00</td>\n",
              "      <td>日経ビジネス・特集　次代を創る100人</td>\n",
              "      <td>2015年12月28日・2016年1月4日合併号に松尾豊特任准教授の記事が掲載されました。</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>https://weblab.t.u-tokyo.ac.jp/%e3%83%86%e3%83...</td>\n",
              "      <td>2016-01-08 15:20:25+00:00</td>\n",
              "      <td>テレビ東京「マネーの羅針盤」に出演しました。</td>\n",
              "      <td>2015年12月26日(土)放送松尾豊特任准教授が「日本の人工知能が世界を変える！？」に出演...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>155 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f24a48b7-d52f-4a5f-9e8a-6ebbf31ca9f0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f24a48b7-d52f-4a5f-9e8a-6ebbf31ca9f0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f24a48b7-d52f-4a5f-9e8a-6ebbf31ca9f0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5efed605-2447-4376-9c87-5a4294dae4d8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5efed605-2447-4376-9c87-5a4294dae4d8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5efed605-2447-4376-9c87-5a4294dae4d8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6cb5c3f8-f471-42af-bf16-5af779dbec57\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6cb5c3f8-f471-42af-bf16-5af779dbec57 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 155,\n  \"fields\": [\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 41,\n        \"samples\": [\n          \"https://weblab.t.u-tokyo.ac.jp/%e9%96%a2-%e5%96%9c%e5%8f%b2%e5%90%9b%e3%81%8c%e8%a8%80%e8%aa%9e%e5%87%a6%e7%90%86%e5%ad%a6%e4%bc%9a%e7%ac%ac24%e5%9b%9e%e5%b9%b4%e6%ac%a1%e5%a4%a7%e4%bc%9a%e3%81%a7%e8%ab%96%e6%96%87%e8%b3%9e/\",\n          \"https://weblab.t.u-tokyo.ac.jp/%e3%80%90%e6%9d%b1%e5%a4%a7%e6%9d%be%e5%b0%be%e7%a0%94%e3%80%912020%e5%b9%b4%e3%81%ae%e6%b4%bb%e5%8b%95%e5%a0%b1%e5%91%8a/\",\n          \"https://weblab.t.u-tokyo.ac.jp/20221130-2/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2016-01-08 15:20:25+00:00\",\n        \"max\": \"2023-08-31 11:28:34+00:00\",\n        \"num_unique_values\": 41,\n        \"samples\": [\n          \"2018-03-12 18:00:29+00:00\",\n          \"2020-12-30 10:55:54+00:00\",\n          \"2022-11-30 18:41:53+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 41,\n        \"samples\": [\n          \"\\u95a2 \\u559c\\u53f2\\u541b\\u304c\\u8a00\\u8a9e\\u51e6\\u7406\\u5b66\\u4f1a\\u7b2c24\\u56de\\u5e74\\u6b21\\u5927\\u4f1a\\u3067\\u8ad6\\u6587\\u8cde\\u3092\\u53d7\\u8cde\\u3057\\u307e\\u3057\\u305f\\u3002\",\n          \"\\u3010\\u6771\\u5927\\u677e\\u5c3e\\u7814\\u30112020\\u5e74\\u306e\\u6d3b\\u52d5\\u5831\\u544a\",\n          \"\\u77e5\\u80fd\\u306e\\u5b9f\\u73fe\\u306b\\u672c\\u6c17\\u3067\\u6311\\u3080\\u3002\\u591a\\u89d2\\u7684\\u306a\\u8996\\u70b9\\u3092\\u6709\\u3059\\u308b\\u3001\\u677e\\u5c3e\\u7814\\u306e\\u7814\\u7a76\\u74b0\\u5883\\u3068\\u306f\\uff1f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 154,\n        \"samples\": [\n          \"\\uff08\\u81ea\\u5df1\\u7d39\\u4ecb\\uff09\\u5ca9\\u6fa4\\u6709\\u7950 / \\u6771\\u4eac\\u5927\\u5b66\\u677e\\u5c3e\\u7814\\u7a76\\u5ba4 \\u8b1b\\u5e2b\\u7d4c\\u6b74\\u5c02\\u9580\\u5206\\u91ce\\u53d7\\u8cde\\u6b74\\u305d\\u306e\\u4ed6\\u306e\\u6d3b\\u52d5\",\n          \"YouTube\\u914d\\u4fe1\\uff1ahttps://youtu.be/GYFba6NVkrs\",\n          \"2015\\u5e7412\\u670828\\u65e5\\u30fb2016\\u5e741\\u67084\\u65e5\\u5408\\u4f75\\u53f7\\u306b\\u677e\\u5c3e\\u8c4a\\u7279\\u4efb\\u51c6\\u6559\\u6388\\u306e\\u8a18\\u4e8b\\u304c\\u63b2\\u8f09\\u3055\\u308c\\u307e\\u3057\\u305f\\u3002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 0,\n        \"max\": 29,\n        \"num_unique_values\": 30,\n        \"samples\": [\n          27,\n          15,\n          23\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb7cdelmFEEn",
        "outputId": "a4f8f7e4-0745-4bac-9b5e-c2ffb00d2818"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "daWH4Tv2EWHp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('intfloat/multilingual-e5-large')"
      ],
      "metadata": {
        "id": "HafyXcvDE8gM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.encode(\"東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？\", normalize_embeddings=True)"
      ],
      "metadata": {
        "id": "waNec9F2FPH6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "`normalize_embeddings=True`  \n",
        "model.encode() 関数がテキストをエンコードした後に、得られた埋め込みベクトル（embeddings）を 正規化 するように指示しています。  \n",
        "ここでいう正規化とは、埋め込みベクトルを L2ノルム で正規化(ベクトルの長さを1に調整すること)することを意味します。  \n",
        "- なぜ正規化するのか？   \n",
        "埋め込みベクトルを正規化する理由は以下の通りです：\n",
        "\n",
        "    - コサイン類似度を計算しやすくする:  \n",
        "     正規化されたベクトルを使うと、コサイン類似度がベクトルの内積と等価になります。コサイン類似度は、ベクトル間の角度を測り、ベクトルの方向のみに基づいて類似性を測るため、正規化はコサイン類似度の計算に非常に役立ちます。\n",
        "\n",
        "    - スケーリングの一貫性:  \n",
        "    埋め込みベクトルのスケールを統一することで、モデルのパフォーマンスを向上させることがあります。特に、ベクトルの長さが異なる場合、正規化しないと、その違いが類似度や距離の計算に影響を与える可能性があります"
      ],
      "metadata": {
        "id": "3S4DZCAOHAf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings.shape)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYmeSoSFFpOh",
        "outputId": "5ce2520b-29ce-4b41-8dfb-aee85863f26d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1024,)\n",
            "[ 0.03941343  0.02392286  0.00169469 ... -0.00920513 -0.0245897\n",
            "  0.01007927]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = [\n",
        "    \"passage: 【東大松尾研 大規模言語モデル講座開講！】LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。\",\n",
        "    \"passage: 【学生限定・受講生募集】9月27日から開講「世界モデルと知能」の受講生を募集します！本講座では，世界モデルを軸に最新の深層学習技術を身につけることができます．深層学習の基礎を習得済みであれば，東大以外の学生も応募可能です！ 締切：9月10日（日）23:59\",\n",
        "    \"passage: 【学生限定・締切間近！】8/25(金)より開講！東大松尾研の金融系PJチームが企画・運営する短期集中講座「金融市場取引と機械学習」の受講生を募集。金融取引に対する機械学習の活用について、理論・実装の両面から学べます。締切:8/7(月)午前10時詳細:\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "iXZhw0_fFvtc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_embeddings = model.encode('query: 東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？', normalize_embeddings=True)"
      ],
      "metadata": {
        "id": "nxC7PK6sF1CO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_embeddings.shape)\n",
        "print(query_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xM9SM0xGOQm",
        "outputId": "20bc771f-e290-452d-f2ea-1c5d68fafa6a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1024,)\n",
            "[ 0.04601318  0.01345694  0.00154629 ... -0.00920955 -0.02119829\n",
            "  0.01422186]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "passage_embeddings = model.encode(input_texts,normalize_embeddings=True)"
      ],
      "metadata": {
        "id": "MjNMShlFGMZ1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(passage_embeddings.shape)\n",
        "print(passage_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwdxwmZuGl9P",
        "outputId": "e307dba7-3b35-4cbe-a817-a1480c89f7aa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 1024)\n",
            "[[ 0.01667397  0.00651591  0.00057924 ... -0.01175364 -0.02482921\n",
            "   0.00795046]\n",
            " [ 0.01795118  0.00137822 -0.00598623 ... -0.01646934 -0.00543556\n",
            "  -0.00059906]\n",
            " [ 0.02234265 -0.0017888  -0.01301421 ... -0.02428826 -0.02211281\n",
            "   0.01296455]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
        "print(scores[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BikuRz-8H2cP",
        "outputId": "2e1f3791-4128-4216-ef7a-2fb4f8840fc7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87.38448333740234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scores.argsort()[::-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zYYie8pTIf3R",
        "outputId": "be927287-cbc6-4bc6-deb3-b0dc3986144c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = [\"passage:\" + content for content in df.content.tolist()]"
      ],
      "metadata": {
        "id": "ogSQiQsfIDyz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = [\n",
        "    \"passage: 【東大松尾研 大規模言語モデル講座開講！】LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。\",\n",
        "    \"passage: 【学生限定・受講生募集】9月27日から開講「世界モデルと知能」の受講生を募集します！本講座では，世界モデルを軸に最新の深層学習技術を身につけることができます．深層学習の基礎を習得済みであれば，東大以外の学生も応募可能です！ 締切：9月10日（日）23:59\",\n",
        "    \"passage: 【学生限定・締切間近！】8/25(金)より開講！東大松尾研の金融系PJチームが企画・運営する短期集中講座「金融市場取引と機械学習」の受講生を募集。金融取引に対する機械学習の活用について、理論・実装の両面から学べます。締切:8/7(月)午前10時詳細:\"\n",
        "    ]\n",
        "query_embeddings = model.encode(['query: 東京大学の松尾研究室が開講する大規模言語モデル講座ではどのような内容を扱いますか？'], normalize_embeddings=True)\n",
        "passage_embeddings = model.encode(input_texts, normalize_embeddings=True)\n",
        "print(passage_embeddings.shape)\n",
        "# 類似度の計算\n",
        "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
        "print(scores[0].tolist())\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "# 類似度の高い順のインデックスを取得\n",
        "print(scores[0].argsort()[::-1])\n",
        "# 一番高い類似度の文章を取得\n",
        "print(input_texts[scores[0].argsort()[::-1][0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-saxszsIk6b",
        "outputId": "084f9e6e-cfd0-4984-a37d-317592cafbc8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 1024)\n",
            "[87.38448333740234, 78.82355499267578, 80.8447036743164]\n",
            "[0 2 1]\n",
            "passage: 【東大松尾研 大規模言語モデル講座開講！】LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- scores[0].argsort()[::-1] は次のような処理を行っています：  \n",
        "    - scores[0] で scores の最初の要素を取得。\n",
        "    - argsort() によって、その要素を昇順に並べるためのインデックスを取得。\n",
        "    - [::-1] で、そのインデックスの順番を**逆順（降順）**に変更。  \n",
        "      \n",
        "つまり、このコードは scores[0] の要素を降順に並べ替えた際のインデックスを取得 するという処理です。"
      ],
      "metadata": {
        "id": "pv9bFhzrKViF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores[0].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZZtzukdJCiH",
        "outputId": "0c2249c6-f580-475e-8b1f-4bfd749e5db6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[87.38448333740234, 78.82355499267578, 80.8447036743164]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHaVGhdCLfJl",
        "outputId": "4c5981a8-fd6d-47bd-e41d-aab8a4793743"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['passage: 【東大松尾研 大規模言語モデル講座開講！】LLMを実装・活用するために必要な知識を扱う無償講座を9/4〜新規開講。GPTの基本的実装からInstruction Tuning/RLHF/高速化等最新のLLMを支える技術まで座学・演習を通じて体系的に学ぶ。募集対象は全国の学生。締切は7月末迄。',\n",
              " 'passage: 【学生限定・受講生募集】9月27日から開講「世界モデルと知能」の受講生を募集します！本講座では，世界モデルを軸に最新の深層学習技術を身につけることができます．深層学習の基礎を習得済みであれば，東大以外の学生も応募可能です！ 締切：9月10日（日）23:59',\n",
              " 'passage: 【学生限定・締切間近！】8/25(金)より開講！東大松尾研の金融系PJチームが企画・運営する短期集中講座「金融市場取引と機械学習」の受講生を募集。金融取引に対する機械学習の活用について、理論・実装の両面から学べます。締切:8/7(月)午前10時詳細:']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation\n"
      ],
      "metadata": {
        "id": "zRQOw6QQKzXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"松尾研ではどのようなロボティクス研究をしていますか？\"\n",
        "query_embeddings = model.encode([\"query:\" + query],normalize_embeddings=True)\n",
        "passage_embeddings = model.encode(input_texts,normalize_embeddings=True)\n",
        "scores = (query_embeddings @ passage_embeddings.T) * 100"
      ],
      "metadata": {
        "id": "Lw0EXVhHJPIF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pmzz5ZhMimR",
        "outputId": "b3fa4889-206a-474b-a166-7f6ecf6b0025"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[81.690544, 75.14685 , 81.21012 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- コサイン類似度"
      ],
      "metadata": {
        "id": "5JB7yZinNlPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "for passage_embedding in passage_embeddings:\n",
        "    dot_product = query_embeddings @ passage_embedding\n",
        "    norm = np.linalg.norm(query_embeddings) * np.linalg.norm(passage_embedding)\n",
        "    similarity = dot_product / norm\n",
        "    print(similarity.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvQpC9nnMnZI",
        "outputId": "efabc91d-fa91-4c72-a58b-ba4dc5d1c4eb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8169054388999939\n",
            "0.7514684796333313\n",
            "0.8121011257171631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "コサイン類似度と内積の値が同じ場合があるのは、ベクトルがすでに**正規化（L2ノルムが1に正規化されている）**されているからです。"
      ],
      "metadata": {
        "id": "Z_X6tCHrNqow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"松尾研ではどのようなロボティクス研究をしていますか？\"\n",
        "query_embeddings = model.encode(['query: ' + query], normalize_embeddings=True)\n",
        "passage_embeddings = model.encode(df.content.tolist(), normalize_embeddings=True)\n",
        "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
        "# 上位3件を表示\n",
        "print('score: ', scores[0][scores[0].argsort()[::-1][0]])\n",
        "print(df.content.tolist()[scores[0].argsort()[::-1][0]])\n",
        "print('score: ', scores[0][scores[0].argsort()[::-1][1]])\n",
        "print(df.content.tolist()[scores[0].argsort()[::-1][1]])\n",
        "print('score: ', scores[0][scores[0].argsort()[::-1][2]])\n",
        "print(df.content.tolist()[scores[0].argsort()[::-1][2]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjDKj7nINpzr",
        "outputId": "53563be3-5b32-4608-d63d-e62c510dece4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score:  89.84386\n",
            "松尾研は人工知能の研究を推進していることはよく知られていますが、ロボティクス研究にも注力していることはご存じない方も多いのではないでしょうか？今回は、そんな松尾研のロボティクス研究の目的やこれまでの経緯、活動の全体像、今後の展望について、修士時代のロボット研究の立ち上げから活躍している博士課程所属 松嶋 達也さんと、ロボットチームのアドバイザーをしている松尾研講師である岩澤 有祐さんにお話を伺いました。\n",
            "score:  89.63023\n",
            "ー松尾研ではなぜロボティクス研究を推進しているのですか？松嶋：松尾研がロボティクス研究を進めるのは、実世界と相互作用を持つ実機を使うことで、松尾研の目標である「知能とは何かを解き明かす」ことに近付くことができると考えるためです。身体を持つシステムとしてのロボットの実装やデータ取得を通じて、汎用的で適応性の高い「かしこい」振る舞いを生み出すことを目的としています。これは、近年、松尾研で力を入れている世界モデルの研究を実世界のデータを用いて行うことに相当します。\n",
            "score:  87.08353\n",
            "松尾研では「知能を創る」というビジョンを掲げ、汎用人工知能研究に注力をしております。知能を工学的に研究するためにはNeuroAIの領域にも拡大が不可欠だと考え、山川 宏先生を中心に研究を進めております。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"松尾研ではどのようなロボティクス研究をしていますか？\"\n",
        "query_embeddings = model.encode([\"query: \"+query],normalize_embeddings=True)\n",
        "scores = (query_embeddings @ passage_embeddings.T) * 100\n",
        "\n",
        "top_k = 2\n",
        "top_k_idx = scores[0].argsort()[::-1][:top_k]\n",
        "\n",
        "top_k_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_51Vfg4NBd5",
        "outputId": "116e3a66-827b-472b-8e7a-b83b3439eace"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_text = f\"\"\"\n",
        "{df.content.tolist()[top_k_idx[0]]}\n",
        "{df.content.tolist()[top_k_idx[1]]}\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"{retrieved_text}\n",
        "上記の文章に基づいて、質問に回答してください。\n",
        "質問: {query}\n",
        "回答:\"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZxvZxGePA1j",
        "outputId": "a368111e-5468-42ee-8ee3-a05aa7804307"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "松尾研は人工知能の研究を推進していることはよく知られていますが、ロボティクス研究にも注力していることはご存じない方も多いのではないでしょうか？今回は、そんな松尾研のロボティクス研究の目的やこれまでの経緯、活動の全体像、今後の展望について、修士時代のロボット研究の立ち上げから活躍している博士課程所属 松嶋 達也さんと、ロボットチームのアドバイザーをしている松尾研講師である岩澤 有祐さんにお話を伺いました。\n",
            "ー松尾研ではなぜロボティクス研究を推進しているのですか？松嶋：松尾研がロボティクス研究を進めるのは、実世界と相互作用を持つ実機を使うことで、松尾研の目標である「知能とは何かを解き明かす」ことに近付くことができると考えるためです。身体を持つシステムとしてのロボットの実装やデータ取得を通じて、汎用的で適応性の高い「かしこい」振る舞いを生み出すことを目的としています。これは、近年、松尾研で力を入れている世界モデルの研究を実世界のデータを用いて行うことに相当します。\n",
            "\n",
            "上記の文章に基づいて、質問に回答してください。\n",
            "質問: 松尾研ではどのようなロボティクス研究をしていますか？\n",
            "回答:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/bilingual-gpt-neox-4b\", use_fast=False)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"rinna/bilingual-gpt-neox-4b\")\n",
        "\n",
        "token_ids = tokenizer.encode(prompt,add_special_tokens=False,return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        token_ids.to(model.device),\n",
        "        max_new_tokens=300,\n",
        "        min_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        pad_token_id = tokenizer.pad_token_id,\n",
        "        bos_token_id = tokenizer.bos_token_id,\n",
        "        eos_token_id = tokenizer.eos_token_id\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5NWoYMMRbKX",
        "outputId": "44a5b56f-bd20-4ad5-e833-6517e30aaac3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_ids.shape)\n",
        "print(output_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8wKuby0Sjd-",
        "outputId": "4e2f84aa-1783-4348-a0e6-e47fb74796a7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 529])\n",
            "tensor([[    7, 13271,  7301,   317, 40827,  3595, 36994, 16666,   317,  3226,\n",
            "         16394, 28383,   319,   294,  8071, 34512,  1158,   619,  6674,   948,\n",
            "         17084,   317,  1906, 12587,  1428,   894,  1283,  7879,   304,  2619,\n",
            "         22256,   567,   650, 34952,   294,  9038, 13271,  7301,   304,  8071,\n",
            "         34512,  1158, 23369,   354, 14960, 10648,   294,  1132,   304,  5757,\n",
            "          2480,   294, 31681, 20141,   859,   294, 23955,  2627,  4704,  1158,\n",
            "           304, 36192,   352, 41608, 23279,  1836,    33,  1223,  5243,    33,\n",
            "         18466, 31665,   294,  4704,  6138, 24134,  3223, 13271,  7301,  5809,\n",
            "           361,  1525,  2787,    33,  1271,  4308,  1276,   323,   603,   508,\n",
            "           324, 42306, 10749,   358,   298,     7,   892, 13271,  7301,   369,\n",
            "         13037,  8071, 34512,  1158, 36994,   468,   304, 18696,   650,  1223,\n",
            "          5243,   316, 13271,  7301,   319,  8071, 34512,  1158, 20318,  1572,\n",
            "           294,  1423,   862,   328, 21581,  1053,  1423,   958, 37232,   326,\n",
            "           294, 13271,  7301,   304,  5204,   361,   325, 33261, 43287,   324,\n",
            "          3214,   659, 47662,   327,  4154,  4354, 22106,  2782, 19788,   570,\n",
            "          3000,   298,  5303,  1053,  1225,  2127,  4704,   304,  9879,   354,\n",
            "          2335,  4951,  4171,   294, 25182, 12293, 17763, 38211,   325, 13798,\n",
            "         18493,   327, 28748, 30751, 45166, 23259,   298,  1030,   294, 12815,\n",
            "           294, 13271,  7301,   326,   948, 20194,   468,   862,  1532,  3595,\n",
            "           324,  1423,  4852,  2335,  5464, 49349, 22779, 15552,   298,     7,\n",
            "             7,  6921, 12446,  9882,   294,  9855,   323, 16391,   431,  4621,\n",
            "           298,     7,  9855,   316,    33, 13271,  7301,   369, 22665,  8071,\n",
            "         34512,  1158,   324, 45887,   567,   650,     7, 16391,   316,    33,\n",
            "         13271,  7301,   369,   294,  1423,   862,   328, 21581,  1053,  1423,\n",
            "           958, 37232,   326,   294, 13271,  7301,   304,  5204,   361,   325,\n",
            "         33261, 43287,   324,  3214,   659, 47662,   327,  4154,  4354, 22106,\n",
            "          2782, 14799, 28383,   298,  5303,  1053,  1225,  2127,  4704,   304,\n",
            "          9879,   354,  2335,  4951,  4171,   294, 25182, 12293, 17763, 38211,\n",
            "           325, 13798, 18493,   327, 28748, 30751, 45166, 23259,   298,  1030,\n",
            "           294, 12815,   294, 13271,  7301,   326,   948, 20194,   468,   862,\n",
            "          1532,  3595,   324,  1423,  4852,  2335,  5464, 49349, 22779, 15552,\n",
            "           298,     7,     7, 13271,  7301,   369,   294,  1423,   862,   328,\n",
            "         21581,  1053,  1423,   958, 37232,   326,   294, 13271,  7301,   304,\n",
            "          5204,   361,   325, 33261, 43287,   324,  3214,   659, 47662,   327,\n",
            "          4154,  4354, 22106,  2782, 14799, 28383,   298,  5303,  1053,  1225,\n",
            "          2127,  4704,   304,  9879,   354,  2335,  4951,  4171,   294, 25182,\n",
            "         12293, 17763, 38211,   325, 13798, 18493,   327, 28748, 30751, 45166,\n",
            "         23259,   298,  1030,   294, 12815,   294, 13271,  7301,   326,   948,\n",
            "         20194,   468,   862,  1532,  3595,   324,  1423,  4852,  2335,  5464,\n",
            "         49349, 22779, 15552,   298,     7,     7, 13271,  7301,   369,   294,\n",
            "          1423,   862,   328, 21581,  1053,  1423,   958, 37232,   326,   294,\n",
            "         13271,  7301,   304,  5204,   361,   325, 33261, 43287,   324,  3214,\n",
            "           659, 47662,   327,  4154,  4354, 22106,  2782, 14799, 28383,   298,\n",
            "          5303,  1053,  1225,  2127,  4704,   304,  9879,   354,  2335,  4951,\n",
            "          4171,   294, 25182, 12293, 17763, 38211,   325, 13798, 18493,   327,\n",
            "         28748, 30751, 45166, 23259,   298,  1030,   294, 12815,   294, 13271,\n",
            "          7301,   326,   948, 20194,   468,   862,  1532,  3595,   324,  1423,\n",
            "          4852,  2335,  5464, 49349, 22779, 15552,   298,     7,     7, 13271,\n",
            "          7301,   369,   294,  1423,   862,   328, 21581,  1053,  1423,   958,\n",
            "         37232,   326,   294, 13271,  7301,   304,  5204,   361,   325, 33261,\n",
            "         43287,   324,  3214,   659, 47662,   327,  4154,  4354, 22106,  2782,\n",
            "         14799, 28383,   298,  5303,  1053,  1225,  2127,  4704,   304,  9879,\n",
            "           354,  2335,  4951,  4171,   294, 25182, 12293, 17763, 38211]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.decode(output_ids.tolist()[0])\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlTRD1sTSsIk",
        "outputId": "f5703bbd-efb7-4fd9-c17c-2971a4f81915"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "松尾研は人工知能の研究を推進していることはよく知られていますが、ロボティクス研究にも注力していることはご存じない方も多いのではないでしょうか?今回は、そんな松尾研のロボティクス研究の目的やこれまでの経緯、活動の全体像、今後の展望について、修士時代のロボット研究の立ち上げから活躍している博士課程所属 松嶋 達也さんと、ロボットチームのアドバイザーをしている松尾研講師である岩澤 有祐さんにお話を伺いました。\n",
            "ー松尾研ではなぜロボティクス研究を推進しているのですか?松嶋:松尾研がロボティクス研究を進めるのは、実世界と相互作用を持つ実機を使うことで、松尾研の目標である「知能とは何かを解き明かす」ことに近付くことができると考えるためです。身体を持つシステムとしてのロボットの実装やデータ取得を通じて、汎用的で適応性の高い「かしこい」振る舞いを生み出すことを目的としています。これは、近年、松尾研で力を入れている世界モデルの研究を実世界のデータを用いて行うことに相当します。\n",
            "\n",
            "上記の文章に基づいて、質問に回答してください。\n",
            "質問: 松尾研ではどのようなロボティクス研究をしていますか?\n",
            "回答: 松尾研では、実世界と相互作用を持つ実機を使うことで、松尾研の目標である「知能とは何かを解き明かす」ことに近付くことができると考えています。身体を持つシステムとしてのロボットの実装やデータ取得を通じて、汎用的で適応性の高い「かしこい」振る舞いを生み出すことを目的としています。これは、近年、松尾研で力を入れている世界モデルの研究を実世界のデータを用いて行うことに相当します。\n",
            "\n",
            "松尾研では、実世界と相互作用を持つ実機を使うことで、松尾研の目標である「知能とは何かを解き明かす」ことに近付くことができると考えています。身体を持つシステムとしてのロボットの実装やデータ取得を通じて、汎用的で適応性の高い「かしこい」振る舞いを生み出すことを目的としています。これは、近年、松尾研で力を入れている世界モデルの研究を実世界のデータを用いて行うことに相当します。\n",
            "\n",
            "松尾研では、実世界と相互作用を持つ実機を使うことで、松尾研の目標である「知能とは何かを解き明かす」ことに近付くことができると考えています。身体を持つシステムとしてのロボットの実装やデータ取得を通じて、汎用的で適応性の高い「かしこい」振る舞いを生み出すことを目的としています。これは、近年、松尾研で力を入れている世界モデルの研究を実世界のデータを用いて行うことに相当します。\n",
            "\n",
            "松尾研では、実世界と相互作用を持つ実機を使うことで、松尾研の目標である「知能とは何かを解き明かす」ことに近付くことができると考えています。身体を持つシステムとしてのロボットの実装やデータ取得を通じて、汎用的で適応性の高い\n"
          ]
        }
      ]
    }
  ]
}